{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "**Due: Mondy, 10/31/2022, 2:15 PM**\n",
    "\n",
    "Welcome to your sixth assignment. You will tune a few hyperparameters in the neural network models and observe the effets of changing them.\n",
    "\n",
    "Contents:\n",
    "\n",
    "1. (30%) Exercise 1: Multi-Layer Neural Network Model\n",
    "2. (10%) Exercise 2.1: Optimizer\n",
    "4. (10%) Exerise 2.2: Batch Size\n",
    "5. (10%) Exercise 2.3: Number of Layers\n",
    "6. (10%) Exercise 2.4: Layer Sizes\n",
    "6. (10%) Exercise 2.5: Learning Rate\n",
    "7. (20%) Exercise 3: Final Tuning\n",
    "\n",
    "Instructions:\n",
    "- The code between the ### START CODE HERE ### and ### END CODE HERE ### comments will be graded.\n",
    "- **Change variable names at your own risk. Make sure you understand what you are doing.**\n",
    "- **For loops can be useful in this assignment.**\n",
    "\n",
    "You will learn:\n",
    "- Create a neural network with general structural constraints.\n",
    "- Effects of some commonly seeing hyperparameters in the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html):\n",
    "``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n",
    "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
    "the ``Dataset``.\n",
    "\n",
    "**If you installed [PyTorch]((https://pytorch.org/get-started/locally/)) using Conda, you may want to [switch Python interpreter](https://code.visualstudio.com/docs/python/environments) to comply with the Conda environment that hosts PyTorch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We are still going to use CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.CIFAR10(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.CIFAR10(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Visulization\n",
    "labels_map = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck',\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    img = torch.permute(img, (1, 2, 0))  # you may want to permute order of the axes\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Generic Multi-Layer Perceptron Model\n",
    "#### **(30%) Exercise 1**: Multi-Layer Neural Network Model \n",
    "Define a neural network model class from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The class shold be able to generate any number of layers and layer sizes neural network model. \n",
    "- Define the layers of the model in `__init__` function according to the input list: `hidden_layer_size`. \n",
    "- **You may want to use a `for` loop** according to the length of `hidden_layer_size` list.\n",
    "\n",
    "> **Hint:** you may find this [post](https://discuss.pytorch.org/t/append-for-nn-sequential-or-directly-converting-nn-modulelist-to-nn-sequential/7104) very useful and this [tutorial](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html) inspiring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        ### START CODE HERE ###\n",
    "        modules = None\n",
    "        ### END CODE HERE ###\n",
    "        self.linear_relu_stack = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 2-layer Test\n",
    "model1 = NeuralNetwork([100])\n",
    "print(model1)\n",
    "X = torch.rand(1, 3, 32, 32)\n",
    "logits = model1(X)  # DO NOT call model.forward()\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"model 1 predicted classes: {y_pred}: {labels_map[int(y_pred)]}\")\n",
    "\n",
    "# 4-layer Test\n",
    "model2 = NeuralNetwork([100, 200, 300])\n",
    "print(model2)\n",
    "X = torch.rand(1, 3, 32, 32)\n",
    "logits = model2(X)  # DO NOT call model.forward()\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"model 2 predicted classes: {y_pred}: {labels_map[int(y_pred)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Expected results:**\n",
    "```console\n",
    "NeuralNetwork(\n",
    "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  (linear_relu_stack): Sequential(\n",
    "    (0): Linear(in_features=3072, out_features=100, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "model 1 predicted classes: tensor([6]): frog\n",
    "NeuralNetwork(\n",
    "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  (linear_relu_stack): Sequential(\n",
    "    (0): Linear(in_features=3072, out_features=100, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=100, out_features=200, bias=True)\n",
    "    (3): ReLU()\n",
    "    (4): Linear(in_features=200, out_features=300, bias=True)\n",
    "    (5): ReLU()\n",
    "    (6): Linear(in_features=300, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "model 2 predicted classes: tensor([4]): deer\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Hyperparameter Tuning\n",
    "Hyperparameters are **NOT** model parameters (weights and biases), but they are important to govern your model's performance. In this section, we are going to experiment following hyperparameters with various settings and to observe their affections on the model prediction accuracy.\n",
    "- Optimizer\n",
    "- Batch Size\n",
    "- Number of Layers\n",
    "- Layer Sizes\n",
    "- Learning Rate\n",
    "\n",
    "#### Training / Test Function\n",
    "The `train()` and `test()` function is pre-defined, so that you can use them for later experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "# Define a test function to evaluate model performance\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    accuracy /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(10%) Exercise 2.1**: Optimizer\n",
    "Plot learning curves (test accuracy) to illustrate how the **optimizer** affects model's performance. \n",
    "- Train at least 5 epochs for each hyperparameter variation.\n",
    "- Use the following 3 optimizers:\n",
    "    1. Stochastic Gradient Descent\n",
    "    2. RMS Propagation\n",
    "    3. Adam\n",
    "- **DO NOT** change other hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=1000)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1000)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_curves = []\n",
    "\n",
    "# SGD\n",
    "### START CODE HERE ###\n",
    "model = None\n",
    "sgd_optimizer = None\n",
    "epochs = None\n",
    "### END CODE HERE ###\n",
    "test_acc = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, sgd_optimizer)\n",
    "    acc = test(test_dataloader, model, loss_fn)\n",
    "    test_acc.append(acc)\n",
    "print(f\"SGD Optimizer Done!\")\n",
    "learning_curves.append(test_acc)\n",
    "\n",
    "# RMSProp\n",
    "### START CODE HERE ###\n",
    "model = None\n",
    "rmsp_optimizer = None\n",
    "epochs = None\n",
    "### END CODE HERE ###\n",
    "test_acc = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, rmsp_optimizer)\n",
    "    acc = test(test_dataloader, model, loss_fn)\n",
    "    test_acc.append(acc)\n",
    "print(f\"RMSProp Optimizer Done!\")\n",
    "learning_curves.append(test_acc)\n",
    "\n",
    "# Adam\n",
    "### START CODE HERE ###\n",
    "model = None\n",
    "adam_optimizer = None\n",
    "epochs = None\n",
    "### END CODE HERE ###\n",
    "test_acc = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, adam_optimizer)\n",
    "    acc = test(test_dataloader, model, loss_fn)\n",
    "    test_acc.append(acc)\n",
    "print(f\"Adam Optimizer Done!\")\n",
    "learning_curves.append(test_acc)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.clf()\n",
    "epoch_ticks = list(range(1, epochs+1))\n",
    "for i in range(len(learning_curves)):\n",
    "    plt.plot(epoch_ticks, learning_curves[i])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(10%) Exercise 2.2**: Batch Size\n",
    "Plot learning curves (test accuracy) to illustrate how the **batch size** affects model performance. \n",
    "- Train at least 5 epochs for each hyperparameter variation.\n",
    "- Use at least 4 different batch sizes.\n",
    "- You can define a list: `batch_sizes` to store all the batch size settings.\n",
    "- `DataLoader` is reusable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_curves = []\n",
    "\n",
    "### START CODE HERE ###\n",
    "batch_sizes = None  \n",
    "epochs = None\n",
    "for bs in batch_sizes:\n",
    "    train_dataloader = None\n",
    "    test_dataloader = None\n",
    "    model = None\n",
    "### END CODE HERE ###\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    test_acc = []\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        acc = test(test_dataloader, model, loss_fn)\n",
    "        test_acc.append(acc)\n",
    "    print(f\"Batch size: {bs} Done!\")\n",
    "    learning_curves.append(test_acc)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.clf()\n",
    "epoch_ticks = list(range(1, epochs+1))\n",
    "for i in range(len(learning_curves)):\n",
    "    plt.plot(epoch_ticks, learning_curves[i])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(10%) Exercise 2.3**: Number of Layers \n",
    "Plot learning curves (test accuracy) to illustrate how **number of layers** affects model performance. \n",
    "- Train at least 5 epochs for each hyperparameter variation.\n",
    "- Use at least 4 different settings.\n",
    "- You can use a list: `layer_sizes` to store all the settings.\n",
    "- Use same size for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=1000)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1000)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_curves = []\n",
    "\n",
    "### START CODE HERE ###\n",
    "layer_sizes = None\n",
    "epochs = None\n",
    "for ls in layer_sizes:\n",
    "    model = None\n",
    "### END CODE HERE ###\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    test_acc = []\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        acc = test(test_dataloader, model, loss_fn)\n",
    "        test_acc.append(acc)\n",
    "    print(f\"Layer size: {ls} Done!\")\n",
    "    learning_curves.append(test_acc)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.clf()\n",
    "epoch_ticks = list(range(1, epochs+1))\n",
    "for i in range(len(learning_curves)):\n",
    "    plt.plot(epoch_ticks, learning_curves[i])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(10%) Exercise 2.4**: Layer Size \n",
    "Plot learning curves (test accuracy) to illustrate how the **layer sizes** affects model performance. \n",
    "- Train at least 5 epochs for each hyperparameter variation.\n",
    "- Use at least 4 different settings.\n",
    "- You can use a list: `layer_sizes` to store all the settings.\n",
    "- Do not change number of layers.\n",
    "- If using 2 or more hidden layers, you can choose different size for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=1000)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1000)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_curves = []\n",
    "\n",
    "### START CODE HERE ###\n",
    "layer_sizes = None\n",
    "epochs = None\n",
    "for ls in layer_sizes:\n",
    "    model = None\n",
    "### END CODE HERE ###\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    test_acc = []\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        acc = test(test_dataloader, model, loss_fn)\n",
    "        test_acc.append(acc)\n",
    "    print(f\"Layer size: {ls} Done!\")\n",
    "    learning_curves.append(test_acc)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.clf()\n",
    "epoch_ticks = list(range(1, epochs+1))\n",
    "for i in range(len(learning_curves)):\n",
    "    plt.plot(epoch_ticks, learning_curves[i])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(10%) Exercise 2.5**: Learning Rate \n",
    "Plot learning curves (test accuracy) to illustrate how the **learning rate** affects model performance. \n",
    "- Train at least 5 epochs for each hyperparameter variation.\n",
    "- Use at least 4 different learning rates.\n",
    "- You can use a list: `learning_rates` to store all the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=1000)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1000)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_curves = []\n",
    "\n",
    "### START CODE HERE ###\n",
    "learning_rates = None\n",
    "epochs = None\n",
    "for lr in learning_rates:\n",
    "    model = None\n",
    "### END CODE HERE ###\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    test_acc = []\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        acc = test(test_dataloader, model, loss_fn)\n",
    "        test_acc.append(acc)\n",
    "    print(f\"Learning Rate: {lr} Done!\")\n",
    "    learning_curves.append(test_acc)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.clf()\n",
    "epoch_ticks = list(range(1, epochs+1))\n",
    "for i in range(len(learning_curves)):\n",
    "    plt.plot(epoch_ticks, learning_curves[i])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Best Performed Model\n",
    "#### **(20%) Exercise 3**: Final Tunings\n",
    "Demonstrate the best performed model using appropriate hyperparameters. \n",
    "\n",
    "> Hint 1: simply combine the previously found best hyperparameters will not guanrantee the best performed model.\n",
    "\n",
    "> Hint 2: Increase `epochs` may also be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "model = None\n",
    "loss_fn = None\n",
    "optimizer =None\n",
    "train_dataloader = None\n",
    "test_dataloader = None\n",
    "epochs = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "learning_curves = []\n",
    "test_acc = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    acc = test(test_dataloader, model, loss_fn)\n",
    "    test_acc.append(acc)\n",
    "print(f\"Optimize Done!\")\n",
    "# learning_curves.append(test_acc)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.clf()\n",
    "epoch_ticks = list(range(1, epochs+1))\n",
    "plt.plot(epoch_ticks, test_acc)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congrats on finishing this assignment! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "111bb55a258b144f6b1581aab35d5df936e18e3736a3cb72eecdb230991dcee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
